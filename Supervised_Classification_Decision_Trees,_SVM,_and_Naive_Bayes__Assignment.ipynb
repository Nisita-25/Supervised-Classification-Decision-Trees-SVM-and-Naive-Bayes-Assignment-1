{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa6iOyv2ZxcK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "-Information Gain in Decision Trees\n",
        "\n",
        "Information Gain is a metric used in decision trees to decide which feature should be used to split the data at each node. It measures how much uncertainty (impurity) in the target variable is reduced after splitting the dataset based on a particular feature.\n",
        "\n",
        "Information Gain is calculated using Entropy, which quantifies the randomness or disorder in the dataset. A dataset with mixed class labels has high entropy, while a pure dataset has low entropy.\n",
        "\n",
        "When building a decision tree, the algorithm:\n",
        "\n",
        "Calculates the entropy of the parent dataset.\n",
        "\n",
        "Splits the dataset using a feature.\n",
        "\n",
        "Computes the weighted entropy of the resulting child subsets.\n",
        "\n",
        "Calculates Information Gain as the difference between parent entropy and child entropy.\n",
        "\n",
        "The feature that provides the highest Information Gain is selected for the split because it best separates the data into distinct classes.\n",
        "\n",
        "Why It Is Important\n",
        "\n",
        "Information Gain helps decision trees:\n",
        "\n",
        "Select the most informative features\n",
        "\n",
        "Create purer child nodes\n",
        "\n",
        "Build efficient and accurate classification models"
      ],
      "metadata": {
        "id": "Szi6rGaoZyby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "-Gini Impurity vs Entropy\n",
        "\n",
        "Gini Impurity and Entropy are two common metrics used in decision trees to measure the impurity or disorder of a dataset, guiding the algorithm in selecting the best feature to split the data. Gini Impurity calculates the probability of misclassifying a randomly chosen instance if it were labeled according to the class distribution. It is computationally simple, fast, and commonly used in CART decision trees. Entropy, on the other hand, comes from information theory and measures the amount of uncertainty or randomness in the dataset. It is slightly more sensitive to changes in class probabilities and is used in algorithms like ID3 and C4.5. While both metrics range from 0 (pure) to maximum impurity, Gini is generally faster to compute, whereas Entropy gives a more precise measure of information gain. In practice, the choice between the two depends on the dataset size and the algorithm being used: Gini is preferred for large datasets due to speed, and Entropy is preferred when a theoretically accurate measure of uncertainty is needed."
      ],
      "metadata": {
        "id": "4BUiPv1sZ_WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3:What is Pre-Pruning in Decision Trees?\n",
        "-Pre-Pruning in Decision Trees\n",
        "\n",
        "Pre-pruning (also called early stopping) is a technique used in decision tree algorithms to stop the growth of the tree early, before it perfectly classifies all training samples. The idea is to prevent the tree from becoming too complex and overfitting the training data.\n",
        "\n",
        "During tree construction, the algorithm evaluates whether to split a node further based on predefined criteria, such as:\n",
        "\n",
        "Maximum depth of the tree\n",
        "\n",
        "Minimum number of samples required to split a node\n",
        "\n",
        "Minimum information gain or impurity decrease threshold\n",
        "\n",
        "If the node does not meet the criteria, the algorithm stops splitting, and the node becomes a leaf node.\n",
        "\n",
        "Advantages of Pre-Pruning\n",
        "\n",
        "Reduces overfitting, improving generalization to unseen data\n",
        "\n",
        "Reduces tree complexity and training time\n",
        "\n",
        "Makes the tree simpler and more interpretable\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "If stopped too early, the tree might underfit, missing important patterns in the data"
      ],
      "metadata": {
        "id": "gyDMPBDJZ_r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "E-pyg5B6bF2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "feature_importances = pd.Series(dt_model.feature_importances_, index=X.columns)\n",
        "feature_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Importances (Gini):\")\n",
        "print(feature_importances)\n",
        "\n",
        "print(\"\\nTop 5 Features:\")\n",
        "print(feature_importances.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsGzoYXdba7J",
        "outputId": "1b80d154-b278-490a-dc55-993216c049e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini):\n",
            "mean concave points        0.705839\n",
            "worst texture              0.114062\n",
            "worst radius               0.070187\n",
            "worst area                 0.036653\n",
            "mean texture               0.022885\n",
            "concave points error       0.017164\n",
            "area error                 0.013563\n",
            "worst smoothness           0.010492\n",
            "concavity error            0.007152\n",
            "smoothness error           0.002004\n",
            "mean compactness           0.000000\n",
            "mean area                  0.000000\n",
            "mean perimeter             0.000000\n",
            "mean radius                0.000000\n",
            "perimeter error            0.000000\n",
            "texture error              0.000000\n",
            "radius error               0.000000\n",
            "mean fractal dimension     0.000000\n",
            "mean concavity             0.000000\n",
            "mean symmetry              0.000000\n",
            "mean smoothness            0.000000\n",
            "compactness error          0.000000\n",
            "symmetry error             0.000000\n",
            "fractal dimension error    0.000000\n",
            "worst perimeter            0.000000\n",
            "worst compactness          0.000000\n",
            "worst concavity            0.000000\n",
            "worst concave points       0.000000\n",
            "worst symmetry             0.000000\n",
            "worst fractal dimension    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Top 5 Features:\n",
            "mean concave points    0.705839\n",
            "worst texture          0.114062\n",
            "worst radius           0.070187\n",
            "worst area             0.036653\n",
            "mean texture           0.022885\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What is a Support Vector Machine (SVM)?\n",
        "-Support Vector Machine (SVM)\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is especially popular for binary classification problems. SVM works by finding the best decision boundary (hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "Key Concepts\n",
        "\n",
        "Hyperplane: A line (in 2D), plane (in 3D), or higher-dimensional surface that separates the classes.\n",
        "\n",
        "Support Vectors: The data points closest to the hyperplane. These points are critical in defining the position and orientation of the hyperplane.\n",
        "\n",
        "Margin: The distance between the hyperplane and the nearest support vectors. SVM aims to maximize this margin, which improves generalization to new data.\n",
        "\n",
        "How SVM Works\n",
        "\n",
        "For linearly separable data: SVM finds the hyperplane that separates the two classes with the largest margin.\n",
        "\n",
        "For non-linear data: SVM uses kernel functions (like RBF, polynomial, or sigmoid) to map the data into higher dimensions where a linear separation is possible.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Effective in high-dimensional spaces\n",
        "\n",
        "Works well when the number of features is greater than the number of samples\n",
        "\n",
        "Robust against overfitting if the margin is maximized\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "Can be computationally expensive for large datasets\n",
        "\n",
        "Choosing the right kernel and hyperparameters can be tricky"
      ],
      "metadata": {
        "id": "obrukR-_bGBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: What is the Kernel Trick in SVM?\n",
        "-Kernel Trick in SVM\n",
        "\n",
        "The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data. Instead of trying to separate the data in the original feature space, the kernel trick maps the data into a higher-dimensional space where a linear hyperplane can separate the classes.\n",
        "\n",
        "How It Works\n",
        "\n",
        "Suppose the data cannot be separated by a straight line in 2D.\n",
        "\n",
        "Using a kernel function, SVM implicitly transforms the data into a higher-dimensional space (e.g., 3D).\n",
        "\n",
        "In this new space, a linear hyperplane can separate the classes.\n",
        "\n",
        "The kernel allows SVM to compute inner products in high-dimensional space without explicitly transforming the data, which saves computation.\n",
        "\n",
        "Common Kernel Functions\n",
        "Kernel\tDescription\n",
        "Linear\tNo transformation; used when data is already linearly separable.\n",
        "Polynomial\tMaps data into polynomial feature space.\n",
        "RBF (Radial Basis Function / Gaussian)\tMaps data into infinite-dimensional space; very flexible.\n",
        "Sigmoid\tSimilar to neural network activation; less commonly used."
      ],
      "metadata": {
        "id": "2MJKQLb8Z_vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"SVM Linear Kernel Accuracy: {accuracy_linear:.4f}\")\n",
        "print(f\"SVM RBF Kernel Accuracy: {accuracy_rbf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgQlEjBDcGTb",
        "outputId": "33a410cc-3790-486e-e320-52b8edeea1c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Linear Kernel Accuracy: 0.9630\n",
            "SVM RBF Kernel Accuracy: 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "-Naïve Bayes Classifier\n",
        "\n",
        "The Naïve Bayes classifier is a probabilistic supervised learning algorithm based on Bayes’ Theorem, used for classification tasks. It predicts the probability that a given input belongs to a particular class based on the conditional probabilities of the features.\n",
        "\n",
        "Bayes’ Theorem\n",
        "\n",
        "Bayes’ Theorem forms the basis of this classifier:\n",
        "\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)⋅P(C)\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "P(C∣X) → Probability of class\n",
        "\n",
        "C given features\n",
        "\n",
        "X (posterior)\n",
        "\n",
        "P(X∣C) → Probability of features given the class (likelihood)\n",
        "\n",
        "\n",
        "P(C) → Prior probability of class\n",
        "P(X) → Probability of the features (evidence)\n",
        "\n",
        "Why is it called \"Naïve\"?\n",
        "\n",
        "It is called “naïve” because the algorithm assumes that all features are independent of each other, given the class label. In real-world datasets, this assumption is often not true, but despite this simplification, Naïve Bayes works surprisingly well in practice.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Simple and fast\n",
        "\n",
        "Performs well on high-dimensional data\n",
        "\n",
        "Works well for text classification (spam detection, sentiment analysis)\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "The independence assumption may not hold in practice, which can affect accuracy\n",
        "\n",
        "Not ideal for datasets with correlated features"
      ],
      "metadata": {
        "id": "w8rg7nCSclE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "-Naïve Bayes classifiers have different variants depending on the type of data and the probability distribution assumed for the features.\n",
        "\n",
        "Gaussian Naïve Bayes is used when the features are continuous numerical values. It assumes that each feature follows a Gaussian (normal) distribution. This makes it suitable for datasets with measurements such as height, weight, or lab values, like the Iris or Breast Cancer datasets.\n",
        "\n",
        "Multinomial Naïve Bayes is designed for count or frequency data. It assumes that the features represent the number of times an event occurs, following a multinomial distribution. This is commonly used in text classification, where features are word counts in documents, such as spam detection or document categorization.\n",
        "\n",
        "Bernoulli Naïve Bayes is used for binary data, where features indicate the presence or absence of a characteristic. It assumes each feature follows a Bernoulli distribution and is ideal for tasks such as checking whether a word appears in an email for spam detection."
      ],
      "metadata": {
        "id": "G8VSiHCtc9KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "caler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Gaussian Naïve Bayes Accuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-dxU3BfcgHn",
        "outputId": "17418aa7-ba39-4b5c-85ce-e71f648c5b7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.935672514619883\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.89      0.91        64\n",
            "           1       0.94      0.96      0.95       107\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.93       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 57   7]\n",
            " [  4 103]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cx9_g-9qeuMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}